{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session with mssql, delta, and hive support enabled\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"sql-server-cdc-with-pyspark\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.sqlserver:mssql-jdbc:9.4.1.jre8,io.delta:delta-core_2.12:1.1.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# secrets included for readability; normally they would be in KeyVault, etc.\n",
    "SRC_USER = \"XXXXXX\"\n",
    "SRC_PWD  = \"XXXXXX\"\n",
    "SRC_HOST = \"XXXXXX\"\n",
    "SRC_DB   = \"XXXXXX\"\n",
    "\n",
    "src_table     = \"customers\"\n",
    "src_table_key = \"customer_id\"\n",
    "\n",
    "delta_table_path = f\"/tmp/{src_table}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of fields from the existing object that we are interested in updating\n",
    "cdc_fields = [x.name for x in spark.sql(f\"select * from {src_table}\").schema.fields]\n",
    "\n",
    "spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:sqlserver://{SRC_HOST}:{SRC_PORT}; database={SRC_DB}; fetchsize=20000\") \\\n",
    "        .option(\"dbtable\", f\"cdc.dbo_{src_table}_CT\") \\\n",
    "        .option(\"user\", SRC_USER) \\\n",
    "        .option(\"password\", SRC_PWD) \\\n",
    "        .option(\"encrypt\", \"true\") \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .option(\"hostNameInCertificate\", \"*.database.windows.net\") \\\n",
    "        .load().createOrReplaceTempView(f\"cdc_{src_table}\")\n",
    "\n",
    "# adjust the CDC data for the latest changes\n",
    "df = spark.sql(f\"\"\"\n",
    "WITH ranked_cdc_data AS (\n",
    "  SELECT \n",
    "    *\n",
    "    ,CAST(CASE WHEN `__$operation` = 1 THEN 1 ELSE 0 END AS BOOLEAN) as deleted \n",
    "--  ,CAST(CASE WHEN `__$operation` = 2 THEN 1 ELSE 0 END AS BOOLEAN) as inserted\n",
    "--  ,CAST(CASE WHEN `__$operation` = 4 THEN 1 ELSE 0 END AS BOOLEAN) as updated\n",
    "    ,ROW_NUMBER() OVER (PARTITION BY {src_table_key} ORDER BY `__$start_lsn` DESC, `__$operation` DESC) rank\n",
    "  FROM \n",
    "    cdc_{src_table}\n",
    "  WHERE `__$operation` != 3\n",
    "),\n",
    "latest_cdc_data AS (\n",
    "  SELECT\n",
    "    *\n",
    "  FROM\n",
    "    ranked_cdc_data\n",
    "  WHERE rank = 1\n",
    "  )\n",
    "select * from latest_cdc_data\n",
    "\"\"\").select(cdc_fields + [\"deleted\"])\n",
    " \n",
    "# get the existing delta object\n",
    "deltaTable = DeltaTable.forPath(spark, delta_table_path )        \n",
    "# and merge in the latest changes, deleting records that have been deleted, \n",
    "# updating ones that have been changed, and inserting ones that aren't deleted\n",
    "deltaTable.alias(\"existing\") \\\n",
    "  .merge(df.alias(\"updates\"), f\"existing.{src_table_key} = updates.{src_table_key}\") \\\n",
    "  .whenMatchedDelete(condition = \"updates.deleted = true\") \\\n",
    "  .whenMatchedUpdateAll() \\\n",
    "  .whenNotMatchedInsertAll(condition = \"updates.deleted = false\") \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from customers\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
